{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e876df30-1a40-455b-9599-05ccc2f64c74",
    "_uuid": "d16ed3cb6b5171b350dd538e39a8dd357385505a"
   },
   "source": [
    "# Introduction to Python and Jupyter Notebooks\n",
    "\n",
    "## Data Manipulation using Pandas\n",
    "\n",
    "This is the reference component to the NMSU Pandas workshop 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "Run the following cell to load (1) the pandas library and (2) the dataset we will be working with today `(Hint: run the code by pushing shift + enter)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a09ab15a-4d94-43d8-8862-9cbcba6898f6",
    "_uuid": "9e10c73b1a2705e83c779a18d8ee7d588d0347df"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pols = pd.read_csv(\"pollutants.csv\", dtype = object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1b98e0b5-afe2-4a81-bbdb-b6dd57087b52",
    "_uuid": "3dfb07294cbda411201270d821c6f75c79e5ef39"
   },
   "source": [
    "Selecting specific values of a `pandas` `DataFrame` or `Series` to work on is an implicit step in almost any data operation you'll run. Hence a solid understanding of how to slice and dice a dataset is vital.\n",
    "\n",
    "We specified dtype = object earlier because there are many different data types present in this data frame- yet all of them our objects.  Now, we need to convert some columns to numeric for manipulation with the numpy library (Pandas uses numpy, numpy uses numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fiscal_year', 'result']\n",
    "pols[cols] = pols[cols].apply(pd.to_numeric, errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df18ef2a-dd0f-4434-af34-4e5d1085157a",
    "_uuid": "8b720ae90f799dd73fa4c84263e4866bf62bf169"
   },
   "source": [
    "## Native accessors\n",
    "\n",
    "Native Python objects provide many good ways of indexing data. `pandas` carries all of these over, which helps make it easy to start with.\n",
    "\n",
    "Consider this `DataFrame` (Hint: run the code by pushing shift + enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "abdd93c5-55b9-4c7d-b556-b5b9c617102a",
    "_uuid": "04fa80b82c517f8093b0002f8b1b21247f56040b"
   },
   "outputs": [],
   "source": [
    "pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e88e5105-a4a0-4f29-83ff-4edd2b9fb55c",
    "_uuid": "5a997cc06323f4290bb86c95cca45f39ca4d64d7"
   },
   "source": [
    "In Python we can access the property of an object by accessing it as an attribute. A `book` object, for example, might have a `title` property, which we can access by calling `book.title`. Columns in a `pandas` `DataFrame` work in much the same way. \n",
    "\n",
    "Hence to access the `fiscal_year` property of our `pols` we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e88186df-ca52-4802-b9be-6f5dca40c1c4",
    "_uuid": "ab668325f2d7190cc7844228a4cbb06ca7f60c14"
   },
   "outputs": [],
   "source": [
    "pols.fiscal_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da9d5427-ec0f-4df2-99a1-6eb869b8eb44",
    "_uuid": "602c300d22fa92a9a8c4e3571e53284a26bf2bd5"
   },
   "source": [
    "If we have a `dict` object in Python, we can access its values using the indexing (`[]`) operator. Again, we can do the same with `pandas` `DataFrame` columns. It \"just works\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0dadc3b1-e496-467e-9769-f96e3ed6aa5f",
    "_uuid": "5a1c5dde263b3495dcb95261f37dbbd313465c88"
   },
   "outputs": [],
   "source": [
    "pols['fiscal_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5c92f2db-20ef-4502-b7ec-f97a8fe02a34",
    "_uuid": "11561d4f40ef826c42e17f3e0de58ca8344ac645"
   },
   "source": [
    "These are the two ways of selecting a specific columnar `Series` out of a `pandas` `DataFrame`. Neither of them is more or less syntactically valid than the other, but the indexing operator `[]` does have the advantage that it can handle column names with reserved characters in them (e.g. if we had a column named `fiscal year` instead of `fiscal_year`, `pols.fiscal year` wouldn't work). For the same reason, people tend to name objects with underscores instead of spaces\n",
    "\n",
    "Doesn't a `pandas` `Series` look kind of like a fancy `dict`? It pretty much is, so it's no surprise that, to drill down to a single specific value, we need only use the indexing operator `[]` once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a80f627f-1585-402b-bfd1-cdcb4d293e0c",
    "_uuid": "d48f80969e3e53a58851460c9bcf389a60069573"
   },
   "outputs": [],
   "source": [
    "pols['fiscal_year'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "29662e95-4596-42eb-8cc8-19457c22a3d7",
    "_uuid": "2a96160b000454e4d82bebb9b414bfcd7e53fb22"
   },
   "source": [
    "## Index-based selection\n",
    "\n",
    "The indexing operator and attribute selection are nice because they work just like they do in the rest of the Python ecosystem. As a novice, this makes them easy to pick up and use. However, `pandas` has its own accessor operators, `loc` and `iloc`. For more advanced operations, these are the ones you're supposed to be using.\n",
    "\n",
    "`pandas` indexing works in one of two paradigms. The first is **index-based selection**: selecting data based on its numerical position in the data. `iloc` follows this paradigm.\n",
    "\n",
    "To select the first row of data in this `DataFrame`, we may use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8ca91e3-de76-4f54-9fc8-50ce917f006d",
    "_uuid": "21d23f8110dc25a1beb684305c4cb6755f8e12c5"
   },
   "outputs": [],
   "source": [
    "pols.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20150f16-38fd-4c2e-8364-dde4389e22b0",
    "_uuid": "6b25a7bd27709279bd9e347e83ed99852eec506e"
   },
   "source": [
    "Both `loc` and `iloc` are row-first, column-second. This is the opposite of what we do in native Python, which is column-first, row-second.\n",
    "\n",
    "This means that it's marginally easier to retrieve rows, and marginally harder to get retrieve columns. To get a column with `iloc`, we can do the following (`We're selecting 3 to get the general_location column- Pandas indexing starts with 0 not 1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7c9b85dc-e11b-4b33-b5dc-f3a4dcf3fd0d",
    "_uuid": "e9eb1af97f599b71d1cd993794b98a430a00d8c7"
   },
   "outputs": [],
   "source": [
    "pols.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "22d5cf6b-9083-4a7d-866e-2313bcc3f94c",
    "_uuid": "61223ecd9c4533bbb8c086c777610a426f56247a"
   },
   "source": [
    "On its own the `:` operator, which also comes from native Python, means \"everything\". When combined with other selectors, however, it can be used to indicate a range of values. For example, to select the `country` column from just the first, second, and third row, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "622c6b8d-cf36-4fc4-921b-b2afb834bac5",
    "_uuid": "7d5dae3b0d0abbbe552a306055167d180c975d50"
   },
   "outputs": [],
   "source": [
    "pols.iloc[:3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "932365f1-7e6f-4003-b99d-c41267a47eae",
    "_uuid": "9352f0e4a72baa09eba0619096ec9fceabb8bf99"
   },
   "source": [
    "Or, to select just the second and third entries, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5dccc9b5-72de-467f-9fbe-76ee19f8e49c",
    "_uuid": "6b5659f0ec3b78b6d63fe55758b41517ed16a3b5"
   },
   "outputs": [],
   "source": [
    "pols.iloc[1:3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e21b49e3-a358-48a0-afd5-014cd24ee2e5",
    "_uuid": "89b8338345eed2a406afbf24d66c0aaf430e77dd"
   },
   "source": [
    "It's also possible to pass a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a721c08b-2d70-4205-9f0f-275e43eb9e41",
    "_uuid": "cfa5c4149759dfeae1625db5651eae8c274d9406"
   },
   "outputs": [],
   "source": [
    "pols.iloc[[0, 1, 2], 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cda24b3c-a007-45f8-b6e5-6a81a5573b35",
    "_uuid": "4d7268098c13d7a5e4d951eb0b08d372a18d2f56"
   },
   "source": [
    "Finally, it's worth knowing that negative numbers can be used in selection. This will start counting forwards from the _end_ of the values. So for example here are the last five elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c086ed13-3a41-4c8e-9d99-19423d736050",
    "_uuid": "ba3e8965ce87c10e00426e28574e72ddcaefd7cc"
   },
   "outputs": [],
   "source": [
    "pols.iloc[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5453ebb3-68d6-4891-a243-b83d7610c3a7",
    "_uuid": "6214fa7cc373d08136c0fd0d1db3cd3200bfd927"
   },
   "source": [
    "## Label-based selection\n",
    "\n",
    "The second paradigm for attribute selection is the one followed by the `loc` operator: **label-based selection**. In this paradigm it's the data index value, not its position, which matters.\n",
    "\n",
    "For example, to get the first entry in `pols`, we would now do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6562e0c-8d67-4125-80c0-d00681cae11a",
    "_uuid": "089c8a3b42b763eda4e6968d3f6f879cf0c8fd37"
   },
   "outputs": [],
   "source": [
    "pols.loc[0, 'fiscal_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "37dacbfd-500c-477e-a880-84c69db9d33e",
    "_uuid": "5bed8e5afda38022558f2e812e50a078f82a2283"
   },
   "source": [
    "`iloc` is conceptually simpler than `loc` because it ignores the dataset's indices. When we use `iloc` we treat the dataset like a big matrix (a list of lists), one that we have to index into by position. `loc`, by contrast, uses the information in the indices to do its work. Since your dataset usually has meaningful indices, it's usually easier to do things using `loc` instead. For example, here's one operation that's much easier using `loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94593b87-a433-4915-9749-99fdb45b69ef",
    "_uuid": "b4b7a69b0b061914d9c8c60f3bf4312ea132d970"
   },
   "outputs": [],
   "source": [
    "pols.loc[:, ['fiscal_year', 'parameter', 'result', 'units']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "237592d4-1ebc-424b-a778-3cc6cd667add",
    "_uuid": "6f41f0ab82a724517b0a0381865d504e38e8e132"
   },
   "source": [
    "When choosing or transitioning between `loc` and `iloc`, there is one \"gotcha\" worth keeping in mind, which is that the two methods use slightly different indexing schemes.\n",
    "\n",
    "`iloc` uses the Python stdlib indexing scheme, where the first element of the range is included and the last one excluded. So 0:10 will select entries 0,...,9. `loc`, meanwhile, indexes inclusively. So 0:10 will select entries 0,...,10.\n",
    "\n",
    "Why the change? Remember that loc can index any stdlib type: strings, for example. If we have a DataFrame with index values `Apples, ..., Potatoes, ...`, and we want to select \"all the alphabetical fruit choices between Apples and Potatoes\", then it's a heck of a lot more convenient to index `df.loc['Apples':'Potatoes']` than it is to index something like `df.loc['Apples', 'Potatoet]` (`t` coming after `s` in the alphabet).\n",
    "\n",
    "This is particularly confusing when the `DataFrame` index is a simple numerical list, e.g. `0,...,1000`. In this case `df.iloc[0:1000]` will return 1000 entries, while `df.loc[0:1000]` return 1001 of them! To get 1000 elements using `loc`, you will need to go one lower and ask for `df.iloc[0:999]`. Earlier versions of this tutorial did not point this out explicitly, leading to a lot of user confusion on some of the related answers, so we've included this note here explaining this issue.\n",
    "\n",
    "Otherwise, the semantics of using `loc` are the same as those for `iloc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec99fc57-9ba6-4e1f-b392-00a359ac3520",
    "_uuid": "6f618788ba7b774ebcb9dbf9106374db635b5e70"
   },
   "source": [
    "## Manipulating the index\n",
    "\n",
    "Label-based selection derives its power from the labels in the index. Critically, the index we use is not immutable. We can manipulate the index in any way we see fit.\n",
    "\n",
    "The `set_index` method can be used to do the job. Here is what happens when we `set_index` to the `result` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0d0352f-297f-44ff-b6d7-c778e027e22b",
    "_uuid": "22b5b3eadffe0e6c7e37d1dc597dd655eae2aa8c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pols.set_index(\"result\", inplace = True)\n",
    "pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfa13a38-ecc9-4a18-8ef6-0a10ba000828",
    "_uuid": "54d49592b7ce821bdaaf2c0a55e574dc0a760b70"
   },
   "source": [
    "This index, however, is not so informative. If you're going to label the rows of your DataFrame, it would be good to label them in a manner which is more meaningful than the current one. Let's restore the original index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols.reset_index(inplace = True)\n",
    "pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a55aa19-17cc-4093-9c90-eea60e2f16b5",
    "_uuid": "600adf843394d74943b7dc3d9208643f1ae5e249"
   },
   "source": [
    "## Data Evaluation\n",
    "\n",
    "So far we've been indexing various strides of data, using structural properties of the `DataFrame` itself. To do *interesting* things with the data, however, we want to find some `answers` \n",
    "\n",
    "Usually I begin by finding as much information about the data as I can. For example, what are the different general locations that are present in the dataset? This requires the numpy function `unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8f7ea1bc-b7e3-4e92-a0a1-933b82fbb392",
    "_uuid": "c1ebf674a3264a30d717b04d0e0db632453c19f2"
   },
   "outputs": [],
   "source": [
    "pols.general_location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a55aa19-17cc-4093-9c90-eea60e2f16b5",
    "_uuid": "600adf843394d74943b7dc3d9208643f1ae5e249"
   },
   "source": [
    "Well, that's quite a list!  How many different places, exactly? This requires the built-in python function `len()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8f7ea1bc-b7e3-4e92-a0a1-933b82fbb392",
    "_uuid": "c1ebf674a3264a30d717b04d0e0db632453c19f2"
   },
   "outputs": [],
   "source": [
    "len(pols.general_location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing data\n",
    "\n",
    "To perform numerical assessments or manipulations, we must account for missing data.  This can be done in a variety of ways.  The simplest, perhaps, is just to drop the rows for which the value is NaN. \n",
    "\n",
    "Let's find the range of fiscal years in this data set. To do this, we must first drop the NA values like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols.dropna(subset = ['fiscal_year'], inplace = True)\n",
    "pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a55aa19-17cc-4093-9c90-eea60e2f16b5",
    "_uuid": "600adf843394d74943b7dc3d9208643f1ae5e249"
   },
   "source": [
    "Next, we use the built-in python functions min() and max() to evaluate the date range as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols.fiscal_year.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols.fiscal_year.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a55aa19-17cc-4093-9c90-eea60e2f16b5",
    "_uuid": "600adf843394d74943b7dc3d9208643f1ae5e249"
   },
   "source": [
    "We could also sort the dataframe by year, then find the first and the last entry as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pols.sort_values('fiscal_year')\n",
    "print(result.fiscal_year.head(1))\n",
    "print(result.fiscal_year.tail(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6a55aa19-17cc-4093-9c90-eea60e2f16b5",
    "_uuid": "600adf843394d74943b7dc3d9208643f1ae5e249"
   },
   "source": [
    "## Conditional selection\n",
    "\n",
    "So far we've been indexing various strides of data, using structural properties of the `DataFrame` itself. To do *interesting* things with the data, however, we often need to ask questions based on conditions. \n",
    "\n",
    "For example, suppose we're interested in finding out ways that pollutants change over time. We don't know how many locations have a range of dates, but we can start by asking where the first tests were done like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols.fiscal_year == 1986"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed3dfcaf-c5ae-403d-8002-6c742e6030aa",
    "_uuid": "86e8f2c051d107ffc2d7a7abb0849644a71a9902"
   },
   "source": [
    "This operation produced a `Series` of `True`/`False` booleans based on the `fiscal_year` of each record.  This result can then be used inside of `loc` to select the relevant data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8aae98b0-b928-49f0-bd44-eda73c5c18e1",
    "_uuid": "4a9a0af542d762da87a5677eebf45128256f611e"
   },
   "outputs": [],
   "source": [
    "pols.loc[pols.fiscal_year == 1986]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all of the locations that were tested in 1986 like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8aae98b0-b928-49f0-bd44-eda73c5c18e1",
    "_uuid": "4a9a0af542d762da87a5677eebf45128256f611e"
   },
   "outputs": [],
   "source": [
    "pols.loc[pols.fiscal_year == 1986].general_location.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can view the test locations ten years later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pols.loc[pols.fiscal_year == 1996].general_location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look, we can see that Tampa Bay is in both of those lists, so we can view how pollutants in Tampa Bay change over time. First we will create a dataframe containing just the Tampa Bay tests like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = pols.loc[pols.general_location == 'Tampa Bay']\n",
    "tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Next we can sort by `result` which corresponds to pollutant concentration and then group by `parameter` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sort_values('result', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = tb.groupby(['parameter', 'units']).head(5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that isn't very interesting.  Let's drop the zeros and try again. We create a new dataframe for tampa bay based on these criteria using `&` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb3 = pols.loc[(pols.general_location == 'Tampa Bay') & (pols.result > 0)]\n",
    "tb3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5961d455-6351-41b1-80c4-5bc6963447c7",
    "_uuid": "1d61a370ebe33f6e26aaf60d4f3e36107a33cf5f"
   },
   "source": [
    "Let's slice out the columns we are interested in like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb3.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb4 = tb3.iloc[: , [0,3,4,11,21]]\n",
    "tb4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sort and group as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "724b0c7b-34bb-4588-8a7d-e95112df23e2",
    "_uuid": "52c3e261e0b11939d6b432d04dffc9540fe13ed7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tb4.sort_values('result', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = tb4.groupby(['parameter', 'units']).head(5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0028aa5-8ce2-4de0-9ebd-2d3709cb5a85",
    "_uuid": "c062f467f0daf051fddde8725258126ce1d753b2"
   },
   "source": [
    "This is still not very clear.  It's time to introduce a very fun pandas function- `pivot_table`- which is a fun way to get an overview of your data really quickly. Let's see which pollutants changed the most through the years of the tampa bay study (`the default function in pivot_table is the mean but we could also aggregate the results by other functions`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tb4.pivot_table(values = 'result', columns = 'fiscal_year', index = 'parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was sort of fun...  but I want to be able to work with the data not just look at it.  Let's turn the pivot table into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tb4.pivot_table(values = 'result', columns = 'fiscal_year', index = 'parameter' ))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out which pollutants changed the most in the Tampa Bay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we first append the dataframe\n",
    "## with the minimim and maximum across all the columns\n",
    "df['small'] = df.min(axis = 1)\n",
    "df['large'] = df.max(axis = 1)\n",
    "## axis = 1 means across columns\n",
    "## the default is axis = 0\n",
    "df.info()\n",
    "## df.info tells you about the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can take the difference between the smallest and largest values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['change'] = df.large - df.small\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['change'] = df['change'].apply(pd.to_numeric, errors = 'coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values('change', ascending = False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to look at specific types of pollutants- for example, all of the PCBs.  We create a new dataframe containing these columns. First we get rid of the missing data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pols.dropna(subset = ['parameter'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we search for parameters that contain the letters `PCB`, and we omit the zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcbs= pols.loc[pols['parameter'].str.contains(\"PCB\") & (pols.result > 0)]\n",
    "pcbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a pivot table `(stored as a data frame)` to evaluate individual PCBs and the test years, to start to look at changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcbDF = pd.DataFrame(pcbs.pivot_table(values = 'result', columns = 'parameter', index = 'fiscal_year' ))\n",
    "pcbDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see when the cummulative PCB values rise and fall. First we take the sum across the dataFrame Columns like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcbDF['all'] = pcbDF.sum(axis = 1)\n",
    "pcbDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort by the `sum` value that we just stored in the `all` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcbDF.sort_values('all', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to determine WHY 2004 (the most PCBs) was more polluted than the previous decade. Were more tests done?  Different types of tests?  Different areas tested? We will go back to our pcb dataframe to look at this information more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh494 = pcbs.loc[(pcbs.fiscal_year == 2004) | (pcbs.fiscal_year == 1994)]\n",
    "## the pipe operater denotes \"or\"\n",
    "oh494 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tests were from 2004?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oh494.loc[oh494.fiscal_year==2004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tests were from 1994?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(oh494.loc[oh494.fiscal_year==1994])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a difference- more than twice the number of tests  What locations were tested in each of those years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh494[oh494.fiscal_year==2004].general_location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh494[oh494.fiscal_year==1994].general_location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake Michigan was tested in both of those years, lets evaluate PCBs there in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM = oh494.loc[oh494.general_location == 'Lake Michigan']\n",
    "LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another pivot table to look at the PCB changes in Lake Michigan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM2 = LM.pivot_table(values = 'result', columns = 'fiscal_year', index = 'parameter' )\n",
    "LM2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which PCB's changed the most in Lake Michigan during that decade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM2.diff(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well, that's great, but how do I work with that data? Let's make a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMfnl = LM2.diff(axis = 1)\n",
    "LMfnl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is this a dataframe?  Let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMfnl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see which PCBs are responsible for the biggest changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LMfnl.sort_values(2004.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well, we have a conclusion: PCB101_90 decreased from 1994 to 2004, while PCB153_132_168 is responsible for the biggest increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I hope you enjoyed reading- come back if you need help with the companion exercises.  Everything you need to know is right here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
